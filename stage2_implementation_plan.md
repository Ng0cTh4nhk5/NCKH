# Stage 2: Automated Credit Scoring Pipeline - Implementation Plan

K·∫ø ho·∫°ch thi·∫øt k·∫ø h·ªá th·ªëng ch·∫•m ƒëi·ªÉm t√≠n d·ª•ng t·ª± ƒë·ªông cho SME v·ªõi kh·∫£ nƒÉng x·ª≠ l√Ω th·ªùi gian th·ª±c v√† m·ªü r·ªông.

> [!NOTE]
> **Vai tr√≤ t√†i li·ªáu**: ƒê√¢y l√† k·∫ø ho·∫°ch t∆∞ v·∫•n (advisory plan). Ng∆∞·ªùi th·ª±c hi·ªán s·∫Ω t·ª± tri·ªÉn khai d·ª±a tr√™n h∆∞·ªõng d·∫´n n√†y.

## Ph·∫°m vi & R√†ng bu·ªôc H·ªá th·ªëng

### Gi·ªõi h·∫°n ƒê·ªãa l√Ω
- **Khu v·ª±c**: Ch·ªâ √°p d·ª•ng cho **Th√†nh ph·ªë H·ªì Ch√≠ Minh**
- **L√Ω do**: ƒê·ªÅ t√†i nghi√™n c·ª©u t·∫≠p trung v√†o c√°c ng√¢n h√†ng th∆∞∆°ng m·∫°i t·∫°i TP.HCM
- **Impact**: 
  - D·ªØ li·ªáu ƒë·ªãa l√Ω (province_code) s·∫Ω c·ªë ƒë·ªãnh l√† HCMC
  - C√≥ th·ªÉ t·∫≠n d·ª•ng d·ªØ li·ªáu kinh t·∫ø vƒ© m√¥ ƒë·∫∑c th√π c·ªßa TP.HCM
  - Location risk score c√≥ th·ªÉ chi ti·∫øt ƒë·∫øn c·∫•p qu·∫≠n/huy·ªán trong HCM

### Gi·ªõi h·∫°n Ng√†nh ngh·ªÅ
- **Scope hi·ªán t·∫°i**: **Ch∆∞a gi·ªõi h·∫°n ng√†nh ngh·ªÅ c·ª• th·ªÉ**
- **Recommend**: Khi tri·ªÉn khai th·ª±c t·∫ø, n√™n ∆∞u ti√™n c√°c ng√†nh:
  - Th∆∞∆°ng m·∫°i (B√°n l·∫ª, B√°n s·ªâ)
  - S·∫£n xu·∫•t quy m√¥ nh·ªè
  - D·ªãch v·ª• (F&B, Logistics)
  - Xu·∫•t nh·∫≠p kh·∫©u

### Quy m√¥ SME
- Doanh nghi·ªáp v·ª´a v√† nh·ªè theo Ngh·ªã ƒë·ªãnh 39/2018/Nƒê-CP
- Doanh thu: < 200 t·ª∑ VNƒê/nƒÉm
- S·ªë lao ƒë·ªông: < 200 ng∆∞·ªùi

## M·ª•c ti√™u to√°n h·ªçc

T√¨m h√†m √°nh x·∫° t·ªëi ∆∞u:

```
≈∑ = f(X_financial, X_alternative, X_demographic)
```

Trong ƒë√≥:
- **≈∑**: X√°c su·∫•t v·ª° n·ª£ (Probability of Default - PD)
- **X**: C√°c vector ƒë·∫∑c tr∆∞ng ƒë√£ ƒë∆∞·ª£c s·ªë h√≥a

---

## 1. Quy tr√¨nh Th·∫©m ƒë·ªãnh T√≠n d·ª•ng Chu·∫©n (Th·ª±c t·∫ø)

### Flowchart Quy tr√¨nh Truy·ªÅn th·ªëng

```mermaid
flowchart TD
    Start([Kh√°ch h√†ng n·ªôp h·ªì s∆° vay]) --> SurveyCheck{Kh·∫£o s√°t<br/>ban ƒë·∫ßu}
    
    SurveyCheck -->|ƒê·∫°t| DocCollect[Thu th·∫≠p h·ªì s∆°]
    SurveyCheck -->|Kh√¥ng ƒë·∫°t| Reject1([T·ª´ ch·ªëi])
    
    DocCollect --> DocVerify{X√°c minh<br/>h·ªì s∆°}
    DocVerify -->|Thi·∫øu/Sai| RequestMore[Y√™u c·∫ßu b·ªï sung]
    RequestMore --> DocCollect
    DocVerify -->|ƒê·∫ßy ƒë·ªß| FinAnalysis[Ph√¢n t√≠ch t√†i ch√≠nh]
    
    FinAnalysis --> RatioCalc[T√≠nh c√°c ch·ªâ s·ªë:<br/>- ROE, ROA<br/>- Current Ratio<br/>- Debt/Equity<br/>- DSCR]
    
    RatioCalc --> CreditHistory[Ki·ªÉm tra l·ªãch s·ª≠<br/>t√≠n d·ª•ng CIC]
    
    CreditHistory --> CollateralCheck[Th·∫©m ƒë·ªãnh<br/>t√†i s·∫£n ƒë·∫£m b·∫£o]
    
    CollateralCheck --> SiteVisit[Kh·∫£o s√°t<br/>th·ª±c ƒë·ªãa]
    
    SiteVisit --> RiskAnalysis[Ph√¢n t√≠ch<br/>r·ªßi ro]
    
    RiskAnalysis --> CreditCommittee{H·ªôi ƒë·ªìng<br/>t√≠n d·ª•ng}
    
    CreditCommittee -->|ƒê·∫°t| Approve[Ph√™ duy·ªát]
    CreditCommittee -->|Kh√¥ng ƒë·∫°t| Reject2([T·ª´ ch·ªëi])
    CreditCommittee -->|C·∫ßn b·ªï sung| RequestMore
    
    Approve --> Disbursement([Gi·∫£i ng√¢n])
    
    style Start fill:#90EE90
    style Disbursement fill:#90EE90
    style Reject1 fill:#FFB6C6
    style Reject2 fill:#FFB6C6
    style CreditCommittee fill:#FFD700
    style RiskAnalysis fill:#FFA500
```

### C√°c ƒëi·ªÉm ra quy·∫øt ƒë·ªãnh ch√≠nh

1. **Kh·∫£o s√°t ban ƒë·∫ßu**: L·ªçc s∆° b·ªô kh√°ch h√†ng kh√¥ng ƒë·ªß ƒëi·ªÅu ki·ªán
2. **X√°c minh h·ªì s∆°**: Ki·ªÉm tra t√≠nh ƒë·∫ßy ƒë·ªß v√† h·ª£p l·ªá
3. **Ph√¢n t√≠ch ch·ªâ s·ªë t√†i ch√≠nh**: ƒê√°nh gi√° nƒÉng l·ª±c t√†i ch√≠nh
4. **L·ªãch s·ª≠ t√≠n d·ª•ng**: Ki·ªÉm tra CIC, n·ª£ x·∫•u
5. **T√†i s·∫£n ƒë·∫£m b·∫£o**: Th·∫©m ƒë·ªãnh gi√° tr·ªã TSƒêB
6. **H·ªôi ƒë·ªìng t√≠n d·ª•ng**: Quy·∫øt ƒë·ªãnh cu·ªëi c√πng

### Th·ªùi gian x·ª≠ l√Ω truy·ªÅn th·ªëng: **5-15 ng√†y l√†m vi·ªác**

---

## 2. Quy tr√¨nh Th·∫©m ƒë·ªãnh T√≠n d·ª•ng T·ª± ƒë·ªông

### Flowchart Quy tr√¨nh T·ª± ƒë·ªông h√≥a

```mermaid
flowchart TD
    Start([API nh·∫≠n d·ªØ li·ªáu]) --> DataValidation{Validation<br/>d·ªØ li·ªáu ƒë·∫ßu v√†o}
    
    DataValidation -->|Invalid| ErrorResponse([Tr·∫£ l·ªói])
    DataValidation -->|Valid| DataEnrich[Data Enrichment:<br/>- G·ªçi API CIC<br/>- Truy v·∫•n d·ªØ li·ªáu n·ªôi b·ªô<br/>- Thu th·∫≠p d·ªØ li·ªáu thay th·∫ø]
    
    DataEnrich --> FeatureExtraction[Feature Extraction:<br/>Tr√≠ch xu·∫•t 50+ features]
    
    FeatureExtraction --> FeatureEngineering[Feature Engineering:<br/>- T√≠nh to√°n ch·ªâ s·ªë<br/>- Chu·∫©n h√≥a<br/>- X·ª≠ l√Ω missing values]
    
    FeatureEngineering --> ModelPrediction[Model Prediction:<br/>Ensemble c·ªßa:<br/>- XGBoost<br/>- Random Forest<br/>- LightGBM]
    
    ModelPrediction --> PDCalculation[T√≠nh PD Score:<br/>≈∑ = f_ensemble_X]
    
    PDCalculation --> RiskSegmentation{Ph√¢n lo·∫°i<br/>r·ªßi ro}
    
    RiskSegmentation -->|PD < 0.05| AutoApprove[Auto-Approve<br/>Low Risk]
    RiskSegmentation -->|0.05 ‚â§ PD < 0.15| ManualReview[Manual Review<br/>Medium Risk]
    RiskSegmentation -->|PD ‚â• 0.15| AutoReject[Auto-Reject<br/>High Risk]
    
    AutoApprove --> LimitCalc[T√≠nh h·∫°n m·ª©c<br/>& l√£i su·∫•t]
    ManualReview --> HumanExpert[Chuy√™n gia<br/>th·∫©m ƒë·ªãnh]
    
    LimitCalc --> Response([Tr·∫£ k·∫øt qu·∫£])
    HumanExpert --> Response
    AutoReject --> Response
    
    Response --> Monitoring[Real-time Monitoring<br/>& Logging]
    
    style Start fill:#90EE90
    style Response fill:#90EE90
    style ErrorResponse fill:#FFB6C6
    style AutoApprove fill:#98FB98
    style AutoReject fill:#FFB6C6
    style ManualReview fill:#FFD700
    style ModelPrediction fill:#87CEEB
```

### Th·ªùi gian x·ª≠ l√Ω t·ª± ƒë·ªông: **< 5 ph√∫t** (Near Real-time)

---

## 3. L·ª±a ch·ªçn ƒê·∫∑c tr∆∞ng (Feature Selection)

### 3.1. X_financial (ƒê·∫∑c tr∆∞ng T√†i ch√≠nh)

#### T·ª´ B√°o c√°o t√†i ch√≠nh

| Feature | C√¥ng th·ª©c | √ù nghƒ©a |
|---------|-----------|---------|
| `revenue_growth` | `(Doanh thu nƒÉm n - Doanh thu nƒÉm n-1) / Doanh thu nƒÉm n-1` | T·ªëc ƒë·ªô tƒÉng tr∆∞·ªüng |
| `profit_margin` | `L·ª£i nhu·∫≠n r√≤ng / Doanh thu` | Kh·∫£ nƒÉng sinh l·ªùi |
| `roa` | `L·ª£i nhu·∫≠n r√≤ng / T·ªïng t√†i s·∫£n` | Hi·ªáu qu·∫£ s·ª≠ d·ª•ng t√†i s·∫£n |
| `roe` | `L·ª£i nhu·∫≠n r√≤ng / V·ªën ch·ªß s·ªü h·ªØu` | Hi·ªáu su·∫•t v·ªën |
| `current_ratio` | `T√†i s·∫£n ng·∫Øn h·∫°n / N·ª£ ng·∫Øn h·∫°n` | Kh·∫£ nƒÉng thanh to√°n |
| `quick_ratio` | `(TSNH - H√†ng t·ªìn kho) / N·ª£ ng·∫Øn h·∫°n` | Thanh kho·∫£n nhanh |
| `debt_to_equity` | `T·ªïng n·ª£ / V·ªën ch·ªß s·ªü h·ªØu` | ƒê√≤n b·∫©y t√†i ch√≠nh |
| `debt_to_asset` | `T·ªïng n·ª£ / T·ªïng t√†i s·∫£n` | T·ª∑ l·ªá n·ª£ |
| `dscr` | `EBITDA / T·ªïng nghƒ©a v·ª• n·ª£ h√†ng nƒÉm` | Kh·∫£ nƒÉng tr·∫£ n·ª£ |
| `inventory_turnover` | `Gi√° v·ªën h√†ng b√°n / H√†ng t·ªìn kho TB` | Hi·ªáu qu·∫£ qu·∫£n l√Ω HTK |
| `receivable_turnover` | `Doanh thu / Kho·∫£n ph·∫£i thu TB` | Hi·ªáu qu·∫£ thu h·ªìi c√¥ng n·ª£ |
| `asset_turnover` | `Doanh thu / T·ªïng t√†i s·∫£n` | Hi·ªáu qu·∫£ s·ª≠ d·ª•ng TS |

#### T·ª´ T√†i kho·∫£n giao d·ªãch

| Feature | M√¥ t·∫£ | C√¥ng th·ª©c |
|---------|-------|-----------|
| `avg_daily_balance` | S·ªë d∆∞ b√¨nh qu√¢n ng√†y (3 th√°ng) | `Œ£(S·ªë d∆∞ cu·ªëi ng√†y) / S·ªë ng√†y` |
| `min_balance_3m` | S·ªë d∆∞ th·∫•p nh·∫•t 3 th√°ng | `min(s·ªë d∆∞)` |
| `cash_flow_volatility` | ƒê·ªô bi·∫øn ƒë·ªông d√≤ng ti·ªÅn | `std(d√≤ng ti·ªÅn h√†ng ng√†y)` |
| `avg_monthly_deposits` | Ti·ªÅn g·ª≠i TB/th√°ng | `Œ£(Ti·ªÅn g·ª≠i) / 3` |
| `avg_monthly_withdrawals` | Ti·ªÅn r√∫t TB/th√°ng | `Œ£(Ti·ªÅn r√∫t) / 3` |
| `net_cash_flow` | D√≤ng ti·ªÅn r√≤ng TB/th√°ng | `avg_deposits - avg_withdrawals` |
| `num_transactions_3m` | S·ªë giao d·ªãch 3 th√°ng | `count(transactions)` |
| `overdraft_count` | S·ªë l·∫ßn th·∫•u chi | `count(balance < 0)` |

### 3.2. X_alternative (ƒê·∫∑c tr∆∞ng Thay th·∫ø)

#### L·ªãch s·ª≠ T√≠n d·ª•ng (CIC)

| Feature | M√¥ t·∫£ |
|---------|-------|
| `cic_score` | ƒêi·ªÉm CIC (n·∫øu c√≥) |
| `num_active_loans` | S·ªë kho·∫£n vay ƒëang c√≥ |
| `total_outstanding_debt` | T·ªïng d∆∞ n·ª£ hi·ªán t·∫°i |
| `max_past_due_days` | S·ªë ng√†y qu√° h·∫°n cao nh·∫•t (12 th√°ng) |
| `num_past_due_30d` | S·ªë l·∫ßn qu√° h·∫°n > 30 ng√†y (12 th√°ng) |
| `num_past_due_90d` | S·ªë l·∫ßn qu√° h·∫°n > 90 ng√†y (12 th√°ng) |
| `debt_burden_ratio` | `T·ªïng d∆∞ n·ª£ / Doanh thu th√°ng` |
| `credit_history_length` | S·ªë nƒÉm c√≥ quan h·ªá t√≠n d·ª•ng |

#### D·ªØ li·ªáu H√†nh vi & Ho·∫°t ƒë·ªông

| Feature | M√¥ t·∫£ |
|---------|-------|
| `business_age` | S·ªë nƒÉm ho·∫°t ƒë·ªông c·ªßa doanh nghi·ªáp |
| `num_employees` | S·ªë l∆∞·ª£ng nh√¢n vi√™n |
| `ownership_stability` | T·ª∑ l·ªá s·ªü h·ªØu ·ªïn ƒë·ªãnh (kh√¥ng ƒë·ªïi ch·ªß) |
| `industry_risk_score` | ƒêi·ªÉm r·ªßi ro ng√†nh ngh·ªÅ (tra b·∫£ng) |
| `district_risk_score` | ƒêi·ªÉm r·ªßi ro theo qu·∫≠n/huy·ªán HCM (1-10) |
| `district_business_density` | M·∫≠t ƒë·ªô DN trong qu·∫≠n (DN/km¬≤) |
| `avg_income_district` | Thu nh·∫≠p b√¨nh qu√¢n qu·∫≠n (VNƒê/th√°ng) |
| `digital_footprint` | M·ª©c ƒë·ªô hi·ªán di·ªán s·ªë (website, social) |
| `supplier_relationships` | S·ªë nh√† cung c·∫•p ch√≠nh |
| `customer_concentration` | T·ª∑ tr·ªçng KH l·ªõn nh·∫•t / Doanh thu |

**B·∫£ng District Risk Score cho TP.HCM** (v√≠ d·ª• minh h·ªça):

| Qu·∫≠n/Huy·ªán | Risk Score | L√Ω do |
|------------|------------|-------|
| Q1, Q3, Q7, B√¨nh Th·∫°nh | 1-2 (Th·∫•p) | Trung t√¢m kinh t·∫ø, thanh kho·∫£n cao |
| Q2, Q9, Th·ªß ƒê·ª©c | 3-4 (TB-Th·∫•p) | Khu c√¥ng nghi·ªáp, ph√°t tri·ªÉn |
| Q4, Q5, Q6, Q8, Q10, Q11 | 4-5 (Trung b√¨nh) | Khu v·ª±c ·ªïn ƒë·ªãnh |
| B√¨nh T√¢n, T√¢n Ph√∫, G√≤ V·∫•p | 5-6 (TB) | Ph√°t tri·ªÉn c√¥ng nghi·ªáp nh·∫π |
| H√≥c M√¥n, C·ªß Chi, Nh√† B√® | 7-8 (TB-Cao) | Ngo·∫°i th√†nh, n√¥ng nghi·ªáp |
| C·∫ßn Gi·ªù | 9-10 (Cao) | Xa trung t√¢m, du l·ªãch sinh th√°i |

### 3.3. X_demographic (ƒê·∫∑c tr∆∞ng Nh√¢n kh·∫©u & Ph√°p l√Ω)

| Feature | M√¥ t·∫£ | Ki·ªÉu | Ghi ch√∫ (HCMC scope) |
|---------|-------|------|----------------------|
| `business_type` | Lo·∫°i h√¨nh DN (TNHH, CP, T∆∞ nh√¢n) | Categorical | - |
| `industry_code` | M√£ ng√†nh VSIC | Categorical | T·∫•t c·∫£ ng√†nh |
| `district_code` | M√£ qu·∫≠n/huy·ªán TP.HCM | Categorical | 24 qu·∫≠n/huy·ªán HCM |
| `business_zone` | Khu v·ª±c KD (CBD, Ngo·∫°i th√†nh) | Categorical | CBD: Q1,3,4,5,10,11 |
| `registered_capital` | V·ªën ƒëi·ªÅu l·ªá | Numeric | - |
| `owner_age` | Tu·ªïi ch·ªß doanh nghi·ªáp | Numeric | - |
| `owner_education` | Tr√¨nh ƒë·ªô h·ªçc v·∫•n | Categorical | - |
| `owner_experience` | S·ªë nƒÉm kinh nghi·ªám | Numeric | - |
| `has_collateral` | C√≥ TSƒêB hay kh√¥ng | Binary | - |
| `collateral_value` | Gi√° tr·ªã TSƒêB | Numeric | - |
| `collateral_location` | V·ªã tr√≠ TSƒêB (qu·∫≠n/huy·ªán) | Categorical | ·∫¢nh h∆∞·ªüng gi√° tr·ªã |
| `loan_to_value` | `H·∫°n m·ª©c vay / Gi√° tr·ªã TSƒêB` | Numeric | - |

---

## 4. Feature Engineering & Normalization

### 4.1. Bi·∫øn ƒë·ªïi d·ªØ li·ªáu

#### Log Transform (cho bi·∫øn l·ªách ph·∫£i)
```python
features_to_log = ['revenue', 'total_assets', 'registered_capital']
X[feature + '_log'] = np.log1p(X[feature])
```

#### T·ª∑ l·ªá & T∆∞∆°ng t√°c
```python
# T∆∞∆°ng t√°c gi·ªØa c√°c bi·∫øn
X['debt_coverage'] = X['dscr'] * X['current_ratio']
X['profitability_index'] = X['profit_margin'] * X['revenue_growth']
```

### 4.2. Chu·∫©n h√≥a (Normalization)

#### Min-Max Scaling (cho features c√≥ ph√¢n ph·ªëi ƒë·ªìng ƒë·ªÅu)
```python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
```

#### Standardization (cho features c√≥ ph√¢n ph·ªëi chu·∫©n)
```python
z = (x - Œº) / œÉ
```

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_standardized = scaler.fit_transform(X)
```

### 4.3. X·ª≠ l√Ω Missing Values

```python
# Median imputation cho numeric features
from sklearn.impute import SimpleImputer

imputer_median = SimpleImputer(strategy='median')
X_numeric_imputed = imputer_median.fit_transform(X_numeric)

# Mode imputation cho categorical
imputer_mode = SimpleImputer(strategy='most_frequent')
X_categorical_imputed = imputer_mode.fit_transform(X_categorical)
```

---

## 5. Ki·∫øn tr√∫c M√¥ h√¨nh (Model Architecture)

### 5.1. Base Models

#### XGBoost
```python
from xgboost import XGBClassifier

xgb_model = XGBClassifier(
    n_estimators=500,
    max_depth=6,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    objective='binary:logistic',
    eval_metric='auc',
    scale_pos_weight=10  # X·ª≠ l√Ω imbalanced data
)
```

#### Random Forest
```python
from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(
    n_estimators=500,
    max_depth=12,
    min_samples_split=20,
    min_samples_leaf=10,
    max_features='sqrt',
    class_weight='balanced'
)
```

#### LightGBM
```python
from lightgbm import LGBMClassifier

lgbm_model = LGBMClassifier(
    n_estimators=500,
    max_depth=8,
    learning_rate=0.05,
    num_leaves=31,
    subsample=0.8,
    colsample_bytree=0.8
)
```

### 5.2. Ensemble Strategy

#### Weighted Average Ensemble
```python
# T√≠nh PD cu·ªëi c√πng
≈∑ = w1 * P_xgb + w2 * P_rf + w3 * P_lgbm

# V·ªõi weights ƒë∆∞·ª£c t·ªëi ∆∞u h√≥a qua validation
w = [0.4, 0.35, 0.25]  # T·ªïng = 1
```

#### Stacking Ensemble
```python
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import StackingClassifier

meta_model = LogisticRegression()

ensemble = StackingClassifier(
    estimators=[
        ('xgb', xgb_model),
        ('rf', rf_model),
        ('lgbm', lgbm_model)
    ],
    final_estimator=meta_model,
    cv=5
)
```

### 5.3. H√†m d·ª± ƒëo√°n cu·ªëi c√πng

```python
def predict_default_probability(X_financial, X_alternative, X_demographic):
    """
    T√≠nh x√°c su·∫•t v·ª° n·ª£ (PD)
    
    Returns:
        ≈∑: Probability of Default [0, 1]
        risk_grade: 'Low', 'Medium', 'High'
        credit_limit: H·∫°n m·ª©c ƒë·ªÅ xu·∫•t
    """
    # 1. Concatenate features
    X = np.concatenate([X_financial, X_alternative, X_demographic], axis=1)
    
    # 2. Feature engineering
    X_engineered = feature_engineering(X)
    
    # 3. Normalize
    X_normalized = scaler.transform(X_engineered)
    
    # 4. Predict with ensemble
    ≈∑ = ensemble.predict_proba(X_normalized)[:, 1]
    
    # 5. Risk grading
    if ≈∑ < 0.05:
        risk_grade = 'Low'
        credit_limit = calculate_limit_low_risk(X)
    elif ≈∑ < 0.15:
        risk_grade = 'Medium'
        credit_limit = calculate_limit_medium_risk(X)
    else:
        risk_grade = 'High'
        credit_limit = 0
    
    return {
        'pd_score': ≈∑,
        'risk_grade': risk_grade,
        'credit_limit': credit_limit,
        'recommendation': get_recommendation(≈∑, risk_grade)
    }
```

---

## 6. Ki·∫øn tr√∫c H·ªá th·ªëng

### 6.1. System Architecture

```mermaid
graph TB
    subgraph "Data Sources"
        A1[Customer Application API]
        A2[Internal Banking System]
        A3[CIC API]
        A4[External Data Providers]
    end
    
    subgraph "Data Layer"
        B1[Data Ingestion Service]
        B2[Data Validation]
        B3[Data Enrichment]
        B4[(Feature Store<br/>PostgreSQL)]
    end
    
    subgraph "Processing Layer"
        C1[Feature Engineering Service]
        C2[Feature Normalization]
        C3[Missing Value Handler]
    end
    
    subgraph "Model Layer"
        D1[XGBoost Service]
        D2[Random Forest Service]
        D3[LightGBM Service]
        D4[Ensemble Aggregator]
    end
    
    subgraph "Decision Layer"
        E1[Risk Grading Engine]
        E2[Credit Limit Calculator]
        E3[Auto-Decision Engine]
    end
    
    subgraph "Output Layer"
        F1[REST API]
        F2[Dashboard UI]
        F3[Monitoring & Logging]
    end
    
    A1 & A2 & A3 & A4 --> B1
    B1 --> B2
    B2 --> B3
    B3 --> B4
    B4 --> C1
    C1 --> C2
    C2 --> C3
    C3 --> D1 & D2 & D3
    D1 & D2 & D3 --> D4
    D4 --> E1
    E1 --> E2
    E2 --> E3
    E3 --> F1 & F2
    F1 & F2 --> F3
    
    style D4 fill:#FFD700
    style E3 fill:#FFA500
    style F3 fill:#87CEEB
```

### 6.2. Technology Stack

| Component | Technology | L√Ω do |
|-----------|-----------|-------|
| **API Gateway** | FastAPI / Flask | High performance, async support |
| **Feature Store** | PostgreSQL + Redis | Persistence + Caching |
| **ML Framework** | Scikit-learn, XGBoost, LightGBM | Industry standard |
| **Model Serving** | MLflow / BentoML | Version control & deployment |
| **Message Queue** | RabbitMQ / Kafka | Async processing, scalability |
| **Monitoring** | Prometheus + Grafana | Real-time metrics |
| **Logging** | ELK Stack | Centralized logging |
| **Container** | Docker + Kubernetes | Orchestration & scaling |

---

## 7. Verification Plan

### 7.1. Automated Tests

#### Unit Tests
```bash
pytest tests/unit/test_feature_engineering.py
pytest tests/unit/test_model_prediction.py
pytest tests/unit/test_risk_grading.py
```

#### Integration Tests
```bash
pytest tests/integration/test_end_to_end_pipeline.py
```

#### Performance Tests
```bash
# Load test: 1000 concurrent requests
locust -f tests/load/locustfile.py --host=http://localhost:8000
```

### 7.2. Model Validation

| Metric | Target | M√¥ t·∫£ |
|--------|--------|-------|
| **AUC-ROC** | > 0.80 | Kh·∫£ nƒÉng ph√¢n bi·ªát default/non-default |
| **Precision** | > 0.75 | ƒê·ªô ch√≠nh x√°c khi d·ª± ƒëo√°n default |
| **Recall** | > 0.70 | T·ª∑ l·ªá ph√°t hi·ªán ƒë∆∞·ª£c default |
| **F1-Score** | > 0.72 | C√¢n b·∫±ng Precision & Recall |
| **KS Statistic** | > 0.40 | Kh·∫£ nƒÉng ph√¢n t√°ch ph√¢n ph·ªëi |

### 7.3. Business Metrics

| Metric | Target | Hi·ªán t·∫°i (Truy·ªÅn th·ªëng) |
|--------|--------|-------------------------|
| **Processing Time** | < 5 ph√∫t | 5-15 ng√†y |
| **Auto-approval Rate** | 30-40% | 0% |
| **Manual Review Rate** | 40-50% | 100% |
| **Throughput** | 10,000 applications/day | ~100 applications/day |
| **Bad Debt Rate** | < 3% | 5-7% |

---

## H∆∞·ªõng d·∫´n Tri·ªÉn khai (Implementation Guidance)

### B∆∞·ªõc 1: Chu·∫©n b·ªã D·ªØ li·ªáu (Data Preparation)

#### 1.1. Thu th·∫≠p d·ªØ li·ªáu TP.HCM
- D·ªØ li·ªáu DN SME t·ª´ S·ªü K·∫ø ho·∫°ch & ƒê·∫ßu t∆∞ TP.HCM
- B√°o c√°o t√†i ch√≠nh m·∫´u (n·∫øu c√≥ t·ª´ ng√¢n h√†ng)
- D·ªØ li·ªáu CIC (n·∫øu ƒë∆∞·ª£c c·∫•p ph√©p)
- D·ªØ li·ªáu kinh t·∫ø vƒ© m√¥ HCM (GDP, thu nh·∫≠p b√¨nh qu√¢n theo qu·∫≠n)

#### 1.2. T·∫°o Synthetic Data (n·∫øu thi·∫øu d·ªØ li·ªáu th·ª±c)
```python
# T·∫°o d·ªØ li·ªáu gi·∫£ l·∫≠p d·ª±a tr√™n ph√¢n ph·ªëi th·ª±c t·∫ø
import pandas as pd
import numpy as np

n_samples = 10000

synthetic_data = pd.DataFrame({
    'revenue': np.random.lognormal(mean=18, sigma=1.5, size=n_samples),  # TB: 200tr/th√°ng
    'district_code': np.random.choice(range(1, 25), size=n_samples),
    'industry_code': np.random.choice(['46', '47', '10', '56'], size=n_samples),  # Th∆∞∆°ng m·∫°i, F&B, S·∫£n xu·∫•t
    # ... th√™m c√°c features kh√°c
    'default_label': np.random.binomial(1, 0.05, size=n_samples)  # 5% default rate
})
```

### B∆∞·ªõc 2: X√¢y d·ª±ng Feature Engineering Pipeline

```python
# feature_engineering.py
class SMEFeatureEngineer:
    def __init__(self):
        self.district_risk_map = {...}  # Map t·ª´ b·∫£ng tr√™n
        
    def engineer_features(self, df):
        # Financial ratios
        df['roa'] = df['net_profit'] / df['total_assets']
        df['current_ratio'] = df['current_assets'] / df['current_liabilities']
        
        # Geography-based
        df['district_risk'] = df['district_code'].map(self.district_risk_map)
        df['is_cbd'] = df['district_code'].isin([1, 3, 4, 5, 10, 11])
        
        # Normalization
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])
        
        return df
```

### B∆∞·ªõc 3: Training Model

```python
# train_model.py
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)

# Train XGBoost
model = XGBClassifier(
    n_estimators=500,
    max_depth=6,
    learning_rate=0.05,
    scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1])  # Handle imbalance
)

model.fit(X_train, y_train, 
          eval_set=[(X_test, y_test)],
          early_stopping_rounds=50,
          verbose=True)

# Evaluate
y_pred_proba = model.predict_proba(X_test)[:, 1]
auc = roc_auc_score(y_test, y_pred_proba)
print(f"AUC-ROC: {auc:.4f}")
```

### B∆∞·ªõc 4: Deployment (Simple API)

```python
# api.py
from fastapi import FastAPI
from pydantic import BaseModel
import pickle

app = FastAPI()

# Load model
model = pickle.load(open('model.pkl', 'rb'))
feature_engineer = pickle.load(open('feature_engineer.pkl', 'rb'))

class CreditRequest(BaseModel):
    revenue: float
    total_assets: float
    district_code: int
    # ... other features

@app.post("/predict")
async def predict_credit_score(request: CreditRequest):
    # Convert to DataFrame
    df = pd.DataFrame([request.dict()])
    
    # Engineer features
    X = feature_engineer.engineer_features(df)
    
    # Predict
    pd_score = model.predict_proba(X)[0, 1]
    
    # Risk grading
    if pd_score < 0.05:
        risk_grade = "Low"
        decision = "Auto-Approve"
    elif pd_score < 0.15:
        risk_grade = "Medium"
        decision = "Manual Review"
    else:
        risk_grade = "High"
        decision = "Auto-Reject"
    
    return {
        "pd_score": float(pd_score),
        "risk_grade": risk_grade,
        "decision": decision
    }
```

---

## Checklist Tri·ªÉn khai

### Phase 1: Research & Design (1-2 tu·∫ßn)
- [ ] Nghi√™n c·ª©u th√™m v·ªÅ credit scoring cho SME
- [ ] Thu th·∫≠p d·ªØ li·ªáu th·ª±c t·∫ø (n·∫øu c√≥) ho·∫∑c chu·∫©n b·ªã synthetic data
- [ ] Finalize feature list d·ª±a tr√™n d·ªØ li·ªáu c√≥ s·∫µn
- [ ] Thi·∫øt k·∫ø database schema cho feature store

### Phase 2: Development (3-4 tu·∫ßn)
- [ ] X√¢y d·ª±ng feature engineering pipeline
- [ ] Train baseline models (XGBoost, Random Forest, LightGBM)
- [ ] Implement ensemble strategy
- [ ] Build prediction API

### Phase 3: Testing & Validation (1-2 tu·∫ßn)
- [ ] Unit tests cho c√°c components
- [ ] Validate model performance (AUC > 0.80)
- [ ] Load testing (throughput, latency)
- [ ] Documentation

### Phase 4: Demo & Presentation (1 tu·∫ßn)
- [ ] Chu·∫©n b·ªã demo v·ªõi d·ªØ li·ªáu m·∫´u
- [ ] T·∫°o dashboard ƒë·ªÉ visualize k·∫øt qu·∫£
- [ ] Vi·∫øt b√°o c√°o k·ªπ thu·∫≠t
- [ ] Present cho th·∫ßy/h·ªôi ƒë·ªìng

---

## T√†i nguy√™n Tham kh·∫£o

### Datasets
- **Public SME Default Datasets**: Kaggle, UCI Machine Learning Repository
- **Vietnam SME Data**: T·ªïng c·ª•c Th·ªëng k√™, S·ªü KHƒêT TP.HCM
- **Industry Benchmarks**: State Bank of Vietnam reports

### Papers & Resources
- "Credit Scoring for SMEs using Machine Learning" (various papers)
- Basel II/III guidelines on credit risk
- Vietnam Central Bank regulations on SME lending

### Tools & Libraries
```bash
# Python environment
pip install pandas numpy scikit-learn
pip install xgboost lightgbm
pip install fastapi uvicorn
pip install mlflow  # For experiment tracking
pip install shap  # For model interpretability
```

---

## Note cu·ªëi c√πng

> [!IMPORTANT]
> Plan n√†y l√† **advisory/t∆∞ v·∫•n**. B·∫°n s·∫Ω t·ª± tri·ªÉn khai t·ª´ng b∆∞·ªõc.
> 
> **C√°c ƒëi·ªÉm c·∫ßn l∆∞u √Ω khi th·ª±c hi·ªán:**
> 1. B·∫Øt ƒë·∫ßu ƒë∆°n gi·∫£n v·ªõi baseline model tr∆∞·ªõc
> 2. Validate t·ª´ng b∆∞·ªõc, ƒë·ª´ng build to√†n b·ªô h·ªá th·ªëng m·ªôt l√∫c
> 3. Focus v√†o feature quality h∆°n l√† model complexity
> 4. Document m·ªçi th·ª© ƒë·ªÉ vi·∫øt b√°o c√°o sau
> 
> **Khi g·∫∑p v·∫•n ƒë·ªÅ, h√£y quay l·∫°i plan n√†y v√† review t·ª´ng b∆∞·ªõc.**

Ch√∫c b·∫°n tri·ªÉn khai th√†nh c√¥ng! üöÄ
